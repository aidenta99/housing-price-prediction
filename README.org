** Handling missing values
There are *27 columns containing missing values*. I investigated on the number of null values of each column to decide which way to handle null values for them. 
- For columns which of most are null, drop them: Alley :              2459 , Pool QC :            2627, Fence :           2118, Misc Feature :       2543
- For columns which have a value of 'typical'/'average', we impute nulls into that value
- For Zoning, I fill nulls based on MS SubClass that it belongs to
- For Lot Frontage, I fill nulls based on Neighborhood that it belongs to
- For the rest, we replace nulls in categorical columns by "None", and in numerical columns by 0 

** Feature engineering
- Some of the non-numeric columns (i.e. 'MS SubClass', 'Yr Sold', 'Mo Sold') are stored as numbers, convert them into strings
- Drop columns which most cells fall into one value 
- Also drop PID because it does not serve any analysis purpose
- Finally, apply dummy transformation to all the categorical variables after handling the missing values
- Since SalePrice is highly skewed, I applied log transform to it to make it more normally distributed.

** Hyper-parameter tuning
For each model, we use GridSearchCV, with scoring = 'neg_mean_absolute_error'. For Lasso and Ridge, I only paid attention to alpha, while for Elastic Net, we tune l1_ratio and alpha. I tried to find alpha that minimise mean absolute error by plotting the graph of -MAE versus alpha (see in img folder). And then choose the range of alpha that makes MAE reaches global minimum as input of GridSearchCV. *Result*:
- Lasso regression: Lasso (alpha=0.0004)
- Ridge regression: Ridge (alpha=24.1)
- Elastic Net: ElasticNet(alpha=0.0006, l1_ratio=0.7)

** Model evaluation
- MSE of Lasso:  158.16
- MAE of Ridge:  157.74
- MAE of Elastic Net:  158.68

- R2 of Lasso:  0.64
- R2 of Ridge:  0.71
- R2 of Elastic Net:  0.54

Based on above evaluation, it is clear that *Ridge (alpha=24.1)* performs best.

** Learning curve
The learning curves of Lasso and Elastic Net clearly show overfitting problem, while the learning curve of Ridge suggests that the two curves come close when training set size is over 100. 
